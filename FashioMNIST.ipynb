{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Pavan Pyla\n",
        "### 22232\n",
        "### ASSIGNMENT -2 Fashion MNISAT Data training and validation\n",
        "\n"
      ],
      "metadata": {
        "id": "FzyWP9pzJ-gS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylyuOcxwHG1U",
        "outputId": "88b8e7fc-d369-457f-9b9e-5d7b3ea7a3f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch.nn as nn                           # Importing the Neccesary Libraries\n",
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "ds_train = datasets.FashionMNIST(\n",
        "    root = 'data',                                          # Downloadint the Training Data set from Inbuilt Datasets\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = ToTensor(),                                # Converting image to tensor\n",
        "    target_transform= Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0,torch.tensor(y), value=1))\n",
        "\n",
        ")\n",
        "\n",
        "test_dataset = datasets.FashionMNIST(\n",
        "    root = 'data',\n",
        "    train = False,\n",
        "    download = True,                         # Downloading test data\n",
        "    transform = ToTensor(),\n",
        "    target_transform= Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0,torch.tensor(y), value=1))\n",
        "\n",
        ")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puqqR9c1HOKx",
        "outputId": "f048c09b-6fe5-45a7-a01d-2b08cc3c02d0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 19158760.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 358231.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 6208787.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 16751184.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "train_size = len(ds_train)                        # Getting the validation set from training data which is 10% of training data\n",
        "val_size = int(0.1 * train_size)\n",
        "\n",
        "train_dataset, val_dataset = random_split(ds_train, [train_size - val_size, val_size])\n",
        "\n",
        "print(f\"Training set size: {len(train_dataset)}\")\n",
        "print(f\"Validation set size: {len(val_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQssuCtZHOGy",
        "outputId": "0300e04c-bcb8-4e85-b6d0-9831b98bb5bf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 54000\n",
            "Validation set size: 6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"Test set size: {len(test_dataset)}\")     # gettting the len of test dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuYPsijCHOEp",
        "outputId": "23d51c07-f9f4-4dba-a855-d1d2878cb3e7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Neural Network"
      ],
      "metadata": {
        "id": "5SsL7LYTKl9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SimpleNet(nn.Module):              # building a Simple Neural Network\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.linear1 = nn.Linear(in_features=28*28, out_features=128)  # Input size 784 (28x28), output size 128\n",
        "        self.linear2 = nn.Linear(in_features=128, out_features=64)   # Input size 128, output size 64\n",
        "        self.linear3 = nn.Linear(in_features=64, out_features=10)    # Input size 64, output size 10 for 10 classes\n",
        "\n",
        "    def forward(self, x): # forward prop function\n",
        "        x = x.view(x.size(0),-1)\n",
        "        x = torch.relu(self.linear1(x))\n",
        "        x = torch.relu(self.linear2(x))\n",
        "        x = torch.softmax(self.linear3(x),dim = 1)\n",
        "        return x\n",
        "\n",
        "\n",
        "simple_net = SimpleNet()   # instance or object creation\n",
        ""
      ],
      "metadata": {
        "id": "LvSrZ_QEHOCB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "3kCRFp9bHN-x"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()  # Taking Cross Entropy Loss as the Loss function\n",
        "optimizer = optim.SGD(simple_net.parameters(), lr=0.01)  # optimizer is SGD stochastic gradient descent"
      ],
      "metadata": {
        "id": "mng0epLAHN3S"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader # importing dataloader"
      ],
      "metadata": {
        "id": "2KQNM_4sHe64"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Validation Function"
      ],
      "metadata": {
        "id": "4DWEsEOaKgsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading all the datasets into dataloader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)             # batch size is 64\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "iySt_wOEHe3K"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainval_loop(train_loader, val_loader, simple_net, criterion, optimizer):\n",
        "\n",
        "    simple_net.train()\n",
        "\n",
        "\n",
        "    size = len(train_loader.dataset)\n",
        "    for batch, (X, y) in enumerate(train_loader):\n",
        "\n",
        "        pred = simple_net(X)\n",
        "        loss = criterion(pred, y)\n",
        "\n",
        "                                                 # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % 100 == 0:\n",
        "            current = batch * len(X)\n",
        "            print(f\"Training loss: {loss.item():>7f}  [{current:>5d}/{size:>5d}]\") #printing training loss\n",
        "\n",
        "\n",
        "    simple_net.eval()\n",
        "\n",
        "    # Validation loop\n",
        "    val_loss = 0\n",
        "    val_size = len(val_loader.dataset)\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(val_loader):\n",
        "            # Compute prediction and loss\n",
        "            pred = simple_net(X)\n",
        "            loss = criterion(pred, y)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            if batch % 100 == 0:\n",
        "                current = batch * len(X)\n",
        "                print(f\"Validation loss: {loss.item():>7f}  [{current:>5d}/{val_size:>5d}]\") #printing validation loss\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"Avg. Validation loss: {avg_val_loss:>7f}\")            #printing average validation loss\n",
        "\n"
      ],
      "metadata": {
        "id": "cQuvpcAgHe0Q"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jXGfn_zrKbMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tessting Function"
      ],
      "metadata": {
        "id": "ziajSoOsKcno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test_loop(test_loader, simple_net, criterion):\n",
        "    simple_net.eval()\n",
        "    size = len(test_loader.dataset)\n",
        "    num_batches = len(test_loader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_loader:\n",
        "            pred = simple_net(X)\n",
        "            test_loss += criterion(pred, y).item()                     # testing loop for accuracy check , basically tessting the model\n",
        "\n",
        "\n",
        "            pred_labels = pred.argmax(dim=1)\n",
        "            y= y.argmax(dim=1)\n",
        "\n",
        "            correct += (pred_labels == y).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    accuracy = correct / size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*accuracy):.2f}%, Avg loss: {test_loss:.8f} \\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QSBLiI9WHexZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calling the Train and validation function"
      ],
      "metadata": {
        "id": "7k2AqODxKUxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "syBciGBLKT6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f'===EPOCH===  {t}')\n",
        "    trainval_loop(train_loader,val_loader, simple_net, criterion, optimizer)      # running train_val loop by 10 times\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KP1Vyk7Hn5A",
        "outputId": "1637ce12-4e33-4db0-fd55-3e80409bb0ec"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===EPOCH===  0\n",
            "Training loss: 1.853493  [    0/54000]\n",
            "Training loss: 1.842747  [ 6400/54000]\n",
            "Training loss: 1.919159  [12800/54000]\n",
            "Training loss: 1.851718  [19200/54000]\n",
            "Training loss: 1.792981  [25600/54000]\n",
            "Training loss: 1.822859  [32000/54000]\n",
            "Training loss: 1.845055  [38400/54000]\n",
            "Training loss: 1.794331  [44800/54000]\n",
            "Training loss: 1.866264  [51200/54000]\n",
            "Validation loss: 1.788117  [    0/ 6000]\n",
            "Avg. Validation loss: 1.801836\n",
            "===EPOCH===  1\n",
            "Training loss: 1.858188  [    0/54000]\n",
            "Training loss: 1.791188  [ 6400/54000]\n",
            "Training loss: 1.794666  [12800/54000]\n",
            "Training loss: 1.815450  [19200/54000]\n",
            "Training loss: 1.765817  [25600/54000]\n",
            "Training loss: 1.790637  [32000/54000]\n",
            "Training loss: 1.760355  [38400/54000]\n",
            "Training loss: 1.702230  [44800/54000]\n",
            "Training loss: 1.733289  [51200/54000]\n",
            "Validation loss: 1.778180  [    0/ 6000]\n",
            "Avg. Validation loss: 1.790135\n",
            "===EPOCH===  2\n",
            "Training loss: 1.823246  [    0/54000]\n",
            "Training loss: 1.825524  [ 6400/54000]\n",
            "Training loss: 1.720633  [12800/54000]\n",
            "Training loss: 1.660027  [19200/54000]\n",
            "Training loss: 1.794837  [25600/54000]\n",
            "Training loss: 1.767054  [32000/54000]\n",
            "Training loss: 1.699714  [38400/54000]\n",
            "Training loss: 1.717056  [44800/54000]\n",
            "Training loss: 1.755986  [51200/54000]\n",
            "Validation loss: 1.758510  [    0/ 6000]\n",
            "Avg. Validation loss: 1.735824\n",
            "===EPOCH===  3\n",
            "Training loss: 1.733645  [    0/54000]\n",
            "Training loss: 1.756553  [ 6400/54000]\n",
            "Training loss: 1.664584  [12800/54000]\n",
            "Training loss: 1.762831  [19200/54000]\n",
            "Training loss: 1.771506  [25600/54000]\n",
            "Training loss: 1.768374  [32000/54000]\n",
            "Training loss: 1.760466  [38400/54000]\n",
            "Training loss: 1.751328  [44800/54000]\n",
            "Training loss: 1.704936  [51200/54000]\n",
            "Validation loss: 1.741083  [    0/ 6000]\n",
            "Avg. Validation loss: 1.718773\n",
            "===EPOCH===  4\n",
            "Training loss: 1.655710  [    0/54000]\n",
            "Training loss: 1.837381  [ 6400/54000]\n",
            "Training loss: 1.732485  [12800/54000]\n",
            "Training loss: 1.745708  [19200/54000]\n",
            "Training loss: 1.728411  [25600/54000]\n",
            "Training loss: 1.714302  [32000/54000]\n",
            "Training loss: 1.797340  [38400/54000]\n",
            "Training loss: 1.790587  [44800/54000]\n",
            "Training loss: 1.682669  [51200/54000]\n",
            "Validation loss: 1.717329  [    0/ 6000]\n",
            "Avg. Validation loss: 1.707587\n",
            "===EPOCH===  5\n",
            "Training loss: 1.788645  [    0/54000]\n",
            "Training loss: 1.755669  [ 6400/54000]\n",
            "Training loss: 1.653138  [12800/54000]\n",
            "Training loss: 1.714048  [19200/54000]\n",
            "Training loss: 1.735985  [25600/54000]\n",
            "Training loss: 1.728399  [32000/54000]\n",
            "Training loss: 1.827174  [38400/54000]\n",
            "Training loss: 1.749753  [44800/54000]\n",
            "Training loss: 1.736439  [51200/54000]\n",
            "Validation loss: 1.703900  [    0/ 6000]\n",
            "Avg. Validation loss: 1.700486\n",
            "===EPOCH===  6\n",
            "Training loss: 1.698465  [    0/54000]\n",
            "Training loss: 1.671322  [ 6400/54000]\n",
            "Training loss: 1.772525  [12800/54000]\n",
            "Training loss: 1.743295  [19200/54000]\n",
            "Training loss: 1.725100  [25600/54000]\n",
            "Training loss: 1.707170  [32000/54000]\n",
            "Training loss: 1.661443  [38400/54000]\n",
            "Training loss: 1.655328  [44800/54000]\n",
            "Training loss: 1.740571  [51200/54000]\n",
            "Validation loss: 1.703085  [    0/ 6000]\n",
            "Avg. Validation loss: 1.694308\n",
            "===EPOCH===  7\n",
            "Training loss: 1.717840  [    0/54000]\n",
            "Training loss: 1.596581  [ 6400/54000]\n",
            "Training loss: 1.665859  [12800/54000]\n",
            "Training loss: 1.688014  [19200/54000]\n",
            "Training loss: 1.702331  [25600/54000]\n",
            "Training loss: 1.695469  [32000/54000]\n",
            "Training loss: 1.631260  [38400/54000]\n",
            "Training loss: 1.644014  [44800/54000]\n",
            "Training loss: 1.646201  [51200/54000]\n",
            "Validation loss: 1.694665  [    0/ 6000]\n",
            "Avg. Validation loss: 1.690126\n",
            "===EPOCH===  8\n",
            "Training loss: 1.735310  [    0/54000]\n",
            "Training loss: 1.678378  [ 6400/54000]\n",
            "Training loss: 1.670616  [12800/54000]\n",
            "Training loss: 1.656038  [19200/54000]\n",
            "Training loss: 1.775302  [25600/54000]\n",
            "Training loss: 1.650556  [32000/54000]\n",
            "Training loss: 1.700869  [38400/54000]\n",
            "Training loss: 1.681226  [44800/54000]\n",
            "Training loss: 1.632618  [51200/54000]\n",
            "Validation loss: 1.687366  [    0/ 6000]\n",
            "Avg. Validation loss: 1.687580\n",
            "===EPOCH===  9\n",
            "Training loss: 1.754070  [    0/54000]\n",
            "Training loss: 1.690315  [ 6400/54000]\n",
            "Training loss: 1.688366  [12800/54000]\n",
            "Training loss: 1.623833  [19200/54000]\n",
            "Training loss: 1.668229  [25600/54000]\n",
            "Training loss: 1.697978  [32000/54000]\n",
            "Training loss: 1.682168  [38400/54000]\n",
            "Training loss: 1.732509  [44800/54000]\n",
            "Training loss: 1.659802  [51200/54000]\n",
            "Validation loss: 1.689604  [    0/ 6000]\n",
            "Avg. Validation loss: 1.683259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loop(test_loader, simple_net, criterion) #test_loop running and checking the results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOG6IMlxHnzI",
        "outputId": "226b3f5b-298d-4136-d301-48947c784931"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error: \n",
            " Accuracy: 78.42%, Avg loss: 1.68926626 \n",
            "\n"
          ]
        }
      ]
    }
  ]
}